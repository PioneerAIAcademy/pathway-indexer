import csv
import datetime
import json
import logging
import os
import re
import shutil
import time

import nest_asyncio
import yaml
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from llama_index.core import SimpleDirectoryReader
from llama_parse import LlamaParse
from markdownify import markdownify as md
from unstructured_client import UnstructuredClient
from unstructured_client.models import shared
from unstructured_client.models.errors import SDKError

from utils.markdown_utils import unstructured_elements_to_markdown
from utils.tools import get_domain, get_files

# Set the logging level to WARNING or higher to suppress INFO messages
logging.basicConfig(level=logging.WARNING)
nest_asyncio.apply()
load_dotenv()


def clean_title(title):
    # replace enters with spaces
    title = title.replace("\n", " ")
    # replace a lot of spaces with one space
    title = " ".join(title.split())
    # trim the text
    title = title.strip()

    return title


def is_empty_content(content):
    content = content.replace("\n", "").replace(" ", "")
    return not content


def clean_markdown(text):
    text = re.sub(r"```markdown+", "", text)

    # Remove Markdown backticks
    text = re.sub(r"```+", "", text)

    # Remove inline code backticks (`text`)
    text = re.sub(r"`+", "", text)

    text = re.sub(r"\[Print\]\(javascript:window\.print\(\)\)", "", text)

    # Remove list of links with same anchors
    text = re.sub(r"(?:(https?:\/\/[^\s]+)\s+){2,}", "", text)  # Remove repeated links

    # Replace [link](#) and [link](url) with link text only
    text = re.sub(r"\[([^\]]+)\]\(([^)]+)\)", r"\1", text)

    # Remove lists of links to the same page (e.g., [All](#) [Web Pages](#))
    text = re.sub(r"(\[([^\]]+)\]\(#\))+(?:\s|,)*", "", text)

    # Regular expression to remove unnecessary text from
    # knowledge base articles
    # Remove specific table headers
    text = re.sub(r"\| \*\*Bot Information\*\* \|\n\| --- \|", "", text)
    text = re.sub(r"\| \*\*Information\*\* \|\n\| --- \|", "", text)
    text = re.sub(r"Views:\n\n\|\s*Article Overview\s*\|\s*\n\|\s*---\s*\|\s*\n\|.*?\|", "", text, flags=re.DOTALL)
    text = re.sub(r"\|\s*Information\s*\|\s*\n\|\s*---\s*\|\s*\n\|.*?\|", "", text, flags=re.DOTALL)
    text = re.sub(r"\|\s*Bot Information\s*\|\s*\n\|\s*---\s*\|\s*\n\|.*?\|", "", text, flags=re.DOTALL)
    text = re.sub(r"\n\s*\*\*Information\*\*\s*\n", "\n", text)
    text = re.sub(r"##? Views:\n\n\| \*\*Article Overview\*\* \|\n\| --- \|\n\|.*?\|", "", text, flags=re.DOTALL)
    text = re.sub(r"Views:\n\n\| \*\*Article Overview\*\* \|\n\| --- \|\n\|.*?\|", "", text, flags=re.DOTALL)
    text = re.sub(r"^\| Information \|\n", "", text, flags=re.MULTILINE)
    text = re.sub(r"\*\s*(Home|Knowledge Base - Home|KA-\d+)\s*\n", "", text)
    text = re.sub(
        r"(You’re offline.*?Knowledge Articles|Contoso, Ltd\.|BYU-Pathway Worldwide|Toggle navigation[.\w\s\*\+\-\:]+|Search Filter|Search\n|Knowledge Article Key:)",
        "",
        text,
    )
    text = re.sub(r"You’re offline\. This is a read only version of the page\.", "", text)

    # Others regular expressions to remove unnecessary text
    # Remove empty headers
    text = re.sub(r"^#+\s*$", "", text, flags=re.MULTILINE)

    # Remove text from WhatsApp navigation
    text = re.sub(r"Copy link\S*", "Copy link", text)

    # Remove text from the hall foundation menu
    # text = re.sub(r"(Skip to content|Menu|[*+-].*)\n", '', text, flags=re.MULTILINE)

    # Remove broken links
    text = re.sub(r"\[([^\]]+)\]\.\n\n\((http[^\)]+)\) \(([^)]+)\)\.", r"\1 (\3).", text)

    # Remove consecutive blank lines
    text = re.sub(r"\n\s*\n\s*\n", "\n\n", text)

    return text


# Helper functions for cleaning and parsing HTML and PDF content
def clean_html(soup):
    """Cleans the HTML content by removing unnecessary elements and extracting the title text."""
    # Extract the title text
    title_text = soup.title.string if soup.title else None

    # Remove unnecessary elements
    for tag in soup([
        "head",
        "style",
        "script",
        "img",
        "svg",
        "meta",
        "link",
        "iframe",
        "noscript",
        "footer",
        "nav",
        "ps-header",
    ]):
        tag.decompose()

    # Create selectors to remove elements
    selectors = [
        '[aria-label="Search Filter"]',
        '[aria-label*="Menu"]',
        '[aria-label*="menu"]',
        '[class*="menu"]',
        '[class*="Menu"]',
        '[role="region"]',
        '[role="dialog"]',
        ".sr-only",
        ".navbar",
        ".breadcrumb",
        ".btn-toolbar",
        ".skip-link",
    ]

    # Remove elements by selectors
    for selector in selectors:
        for tag in soup.select(selector):
            tag.decompose()
    # Determine the content container (main or body)
    content = soup.main or soup.body

    if content and title_text:
        # Create a title header and insert it at the beginning
        title_header = soup.new_tag("title")
        title_header.string = title_text
        content.insert(0, title_header)

    return content or soup  # Return the cleaned content or the entire soup as a fallback


def clean_text(text):
    """
    Cleans the input text by performing the following operations:
    - Replaces null characters with 'th'.
    - Removes square brackets and quotes.
    - Trims leading and trailing whitespace.

    Parameters:
    text (str): The input text to clean.

    Returns:
    str: The cleaned text.
    """
    if not isinstance(text, str):
        return ""

    # Replace null characters
    text = re.sub(r"\x00", "th", text)

    # Remove leading and trailing whitespace
    text = text.strip()

    # Remove leading and trailing square brackets
    if text.startswith("[") and text.endswith("]"):
        text = text[1:-1].strip()

    # Remove leading and trailing quotes (both single and double)
    if (text.startswith("'") and text.endswith("'")) or (text.startswith('"') and text.endswith('"')):
        text = text[1:-1].strip()
        text = text.replace("'", "").replace(",", " |").replace("\n", " ")

    return text


def parse_pdf_to_txt(filepath, out_folder):
    """
    Parse PDF file to a text file.
    """
    s = UnstructuredClient(
        api_key_auth=os.environ["UNSTRUCTURED_API_KEY"],
        server_url=os.environ["UNSTRUCTURED_SERVER_URL"],
    )

    file_path = filepath
    print("Processing PDF file:", file_path)

    with open(file_path, "rb") as f:
        files = shared.Files(content=f.read(), file_name=file_path)

    req = shared.PartitionParameters(
        files=files,
        strategy="fast",
        languages=["eng"],
        encoding="utf-8",
    )

    try:
        resp = s.general.partition(req)
    except SDKError as e:
        print(e)
        return "Error"
    except Exception as e:
        print("Another exception", e)
        return "Error"

    simple_md = unstructured_elements_to_markdown(resp.elements)
    simple_md = clean_text(simple_md)

    if not simple_md:
        return "Error"

    # filepath["size"] = len(simple_md)
    file_out = os.path.join(
        out_folder, "from_pdf", os.path.basename(file_path).replace(".pdf", ".txt")
    )  # filepath["path"].replace(".pdf", ".txt")

    os.makedirs(os.path.dirname(file_out), exist_ok=True)

    with open(file_out, "w", encoding="utf-8") as f:
        f.write(simple_md)

    print(f"Parsed PDF to TXT and saved to: {file_out}")
    return file_out


def convert_html_to_markdown(file_path, out_folder):
    """
    Converts HTML content from a file to Markdown and saves it to a new .txt file.
    """
    with open(file_path, encoding="utf-8") as f:
        html_content = f.read()

    soup = BeautifulSoup(html_content, "html.parser")
    cleaned_soup = clean_html(soup)

    title = soup.contents[0]
    title_tag = title.text if title.name == "title" else ""
    if title_tag:
        title.decompose()

    markdown_content = md(str(cleaned_soup), heading_style="ATX")
    markdown_content = re.sub(r"\n{2,}", "\n\n", markdown_content)

    file_out = os.path.join(out_folder, "from_html", os.path.basename(file_path).replace(".html", ".txt"))
    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(file_out), exist_ok=True)

    if is_empty_content(markdown_content):
        return file_path, "Error parsing."

    with open(file_out, "w", encoding="utf-8") as f:
        f.write(markdown_content)

    print(f"Converted HTML to TXT and saved to: {file_out}")

    return file_out, title_tag


def create_file_extractor(parse_type="pdf"):
    """Create a file extractor based on the parsing type (pdf or html)"""

    if parse_type == ".pdf":
        parser = LlamaParse(
            api_key=os.environ["LLAMA_CLOUD_API_KEY"],
            result_type="markdown",
            parsing_instruction=(
                "Convert the provided text into accurate and well-structured Markdown format, closely resembling the original PDF structure. "
                "Use headers from H1 to H3, with H1 for main titles, H2 for sections, and H3 for subsections. "
                "Detect any bold, large, or all-uppercase text as headers. "
                "Preserve bullet points and numbered lists with proper indentation to reflect nested lists. "
                "if it is not a header, ensure that bold and italic text is properly formatted using double **asterisks** for bold and single *asterisks* for italic"
                "Detect and correctly format blockquotes using the '>' symbol for any quoted text. "
                "When processing text, pay attention to line breaks that may incorrectly join or split words. "
                "Automatically correct common errors, such as wrongly concatenated words or broken lines, to ensure the text reads naturally"
                "If code snippets or technical commands are found, enclose them in triple backticks ``` for proper formatting. "
                "If any tables are detected, parse them as a title (bold header) followed by list items"
                "If you see the same header multiple times, merge them into one."
                "If images contain important text, transcribe only the highlighted or boxed text and ignore general background text. "
                "Do not enclose fragments of code/Markdown or any other content in triple backticks unless they are explicitly formatted as code blocks in the original text. "
                "The final output should be a clean, concise Markdown document closely reflecting the original PDF's intent and structure without adding any extra text."
            ),
        )
    if parse_type == ".html":
        parser = LlamaParse(
            api_key=os.environ["LLAMA_CLOUD_API_KEY"],
            result_type="markdown",  # "markdown" and "text" are available
            parsing_instruction=(
                "Convert the provided text into accurate and well-structured Markdown format, strictly preserving the original structure. "
                "Use headers from H1 to H3 only where they naturally occur in the text, and do not create additional headers or modify existing ones. "
                "Do not split the text into multiple sections or alter the sequence of content. "
                "Detect bold, large, or all-uppercase text as headers only if they represent a natural section break in the original text. "
                "Preserve all links, ensuring that they remain correctly formatted and in their original place in the text. "
                "Maintain bullet points and numbered lists with proper indentation to reflect any nested lists, ensuring list numbers remain in sequence. "
                "If the text is not a header, ensure that bold and italic text is properly formatted using double **asterisks** for bold and single *asterisks* for italic. "
                "Detect and correctly format blockquotes using the '>' symbol for any quoted text, but do not reformat text that is already in correct Markdown format. "
                "Respect the original line breaks and text flow, avoiding unnecessary splits, merges, or reordering of content. "
                "If any tables are detected, parse them as a title (bold header) followed by list items, but do not reformat existing Markdown tables. "
                "Merge identical headers only if they represent the same section and their content is identical, ensuring no changes to the order of the text. "
                "Do not enclose fragments of code/Markdown or any other content in triple backticks unless they are explicitly formatted as code blocks in the original text. "
                "Ensure that the final output is a clean, concise Markdown document that closely reflects the original text's intent and structure, without adding or omitting any content."
            ),
        )
    file_extractor = {".txt": parser}
    return file_extractor


def has_markdown_tables(content):
    """Check if content contains markdown tables"""
    table_patterns = [
        r"\|.*\|.*\|",  # Table row with cells
        r"\|[\s-]*\|[\s-]*\|",  # Table header separator
    ]
    return all(re.search(pattern, content, re.MULTILINE) for pattern in table_patterns)


def parse_txt_to_md(
    file_path, file_extension, stats, empty_llamaparse_files_counted, detailed_log_path, title_tag="", url=None
):
    """
    Parses a .txt file to a Markdown (.md) file using LlamaParse, with detailed logging.
    """
    import datetime
    import json

    with open(file_path, encoding="utf-8") as f:
        content = f.read()

    log_entry = {
        "timestamp": datetime.datetime.now().isoformat(),
        "stage": "parse_txt_to_md",
        "filepath": file_path,
        "status": "START",
        "message": "Starting TXT to MD parsing.",
    }
    if detailed_log_path:
        with open(detailed_log_path, "a") as f:
            f.write(json.dumps(log_entry) + "\n")

    if not has_markdown_tables(content):
        documents = SimpleDirectoryReader(
            input_files=[file_path], file_extractor=create_file_extractor(file_extension)
        ).load_data()
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "stage": "parse_txt_to_md",
            "filepath": file_path,
            "status": "LLAMAPARSE_USED",
            "message": "Used LlamaParse extractor for TXT file.",
        }
        if detailed_log_path:
            with open(detailed_log_path, "a") as f:
                f.write(json.dumps(log_entry) + "\n")
    else:
        documents = SimpleDirectoryReader(input_files=[file_path]).load_data()
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "stage": "parse_txt_to_md",
            "filepath": file_path,
            "status": "DIRECT_LOAD",
            "message": "Loaded TXT file directly without LlamaParse.",
            "url": url,  # Include the URL in the log entry
        }
        if detailed_log_path:
            with open(detailed_log_path, "a") as f:
                f.write(json.dumps(log_entry) + "\n")

    final_content = "\n\n".join([doc.text for doc in documents])

    # If content from LlamaParse is empty, revert to original content
    if is_empty_content(final_content):
        final_content = content

    # If the final content (even after reverting) is empty, then it's a failure
    if is_empty_content(final_content):
        print(f"Final content for {os.path.basename(file_path)} is empty. Moving to error.")
        return True  # True indicates failure/empty

    final_content = "\n\n".join([doc.text for doc in documents])

    # If content from LlamaParse is empty, revert to original content
    if is_empty_content(final_content):
        final_content = content

    # If the final content (even after reverting) is empty, then it's a failure
    if is_empty_content(final_content):
        print(f"Final content for {os.path.basename(file_path)} is empty. Moving to error.")
        return True  # True indicates failure/empty

    # base_filename = os.path.basename(file_path)
    out_name = file_path.replace(".txt", ".md")

    title_tag = clean_title(title_tag)

    with open(out_name, "w", encoding="utf-8") as f:
        if title_tag:
            f.write(f"title: {title_tag}\n")
        f.write(final_content)
        print(f"Parsed TXT to MD and saved to: {out_name}")

    stats["md_files_generated"] += 1

    log_entry = {
        "timestamp": datetime.datetime.now().isoformat(),
        "stage": "parse_txt_to_md",
        "filepath": file_path,
        "status": "FINISHED",
        "message": f"Finished TXT to MD parsing. Empty: {is_empty_content(final_content)}",
    }
    if detailed_log_path:
        with open(detailed_log_path, "a") as f:
            f.write(json.dumps(log_entry) + "\n")

    return False


def associate_markdown_with_metadata(markdown_dirs, csv_path, excluded_domains):
    """
    Associates Markdown files with metadata from a CSV file.

    Parameters:
    - markdown_dirs (list): List of directories containing Markdown files.
    - csv_path (str): Path to the CSV file containing metadata.

    Returns:
    - dict: Mapping of Markdown file paths to their corresponding metadata.
    """
    all_files = get_files(markdown_dirs)
    # Read the CSV file and store the file paths, URLs, headings, and subheadings in a dictionary
    file_metadata_mapping = {}
    with open(csv_path, newline="", encoding="utf-8") as file:
        reader = csv.DictReader(file)
        for row in reader:
            # Extract the filename without the extension and use it as the key
            filename_with_ext = os.path.basename(row["filename"])
            filename_without_ext = os.path.splitext(filename_with_ext)[0]

            # Store metadata using filename without extension as the key
            file_metadata_mapping[filename_without_ext] = {
                "url": row["URL"],
                "heading": clean_text(row["Section"]),
                "subheading": (clean_text(row["Subsection"]) if clean_text(row["Subsection"]) != "Missing" else ""),
                "title": clean_text(row["Title"]),
                "role": row["Role"],
            }

    # Now go through the markdown files in each directory and associate them with the metadata
    markdown_metadata_mapping = {}
    # List to save files without metadata
    no_metadata = []

    for markdown_path in all_files:
        # Get the markdown filename without the extension
        markdown_filename_without_ext = os.path.splitext(os.path.basename(markdown_path))[0]

        # Check if the filename matches any entry in the CSV dictionary
        if markdown_filename_without_ext in file_metadata_mapping:
            # Store the path relative to the directory
            # full_path = os.path.join(markdown_dir, markdown_filename)
            markdown_metadata_mapping[markdown_path] = file_metadata_mapping[markdown_filename_without_ext]
            # open the file, and read if the first line begins with "title: "
            with open(markdown_path, encoding="utf-8") as file:
                lines = file.readlines()

            if len(lines) == 0:
                continue
            # Revisar si la primera línea contiene el título
            first_line = lines[0].strip()

            # get the url from the metadata
            url = markdown_metadata_mapping[markdown_path]["url"]
            if first_line.startswith("title: "):
                # Extraer el título de la primera línea
                title = first_line.replace("title: ", "")
                if get_domain(url) not in excluded_domains:
                    markdown_metadata_mapping[markdown_path]["title_tag"] = title

                # Eliminar la primera línea (la que contiene el título)
                lines = lines[1:]

                # Guardar el archivo sin la primera línea
                with open(markdown_path, "w", encoding="utf-8") as file:
                    file.writelines(lines)

            # clean the markdown file and save it
            with open(markdown_path, encoding="utf-8") as file:
                content = file.read()
                content = clean_markdown(content)
            with open(markdown_path, "w", encoding="utf-8") as file:
                file.write(content)

        else:
            print(f"No metadata found for {markdown_path}. Skipping.")
            no_metadata.append(markdown_path)

    # Guardamos en CSV las rutas de Markdown sin metadata
    no_metadata_csv_path = os.path.join(os.path.dirname(csv_path), "no_metadata.csv")
    with open(no_metadata_csv_path, mode="w", newline="", encoding="utf-8") as nm_file:
        writer = csv.writer(nm_file)
        writer.writerow(["markdown_path"])
        for nm_path in no_metadata:
            writer.writerow([nm_path])

    print("\nMarkdown files and their metadata:")
    for path, meta in markdown_metadata_mapping.items():
        print(f"{path}: {meta}")

    return markdown_metadata_mapping


def remove_existing_yaml_frontmatter(content):
    """
    Removes existing YAML front matter from the given content.
    Assumes that front matter is enclosed between '---' markers.
    """
    yaml_pattern = re.compile(r"^---[\s\S]*?---\s", re.MULTILINE)
    return re.sub(yaml_pattern, "", content, count=1)


def attach_metadata_to_markdown_directories(markdown_dirs, metadata_dict):
    """
    Attaches metadata as YAML front matter to Markdown files.

    Parameters:
    - markdown_dirs (list): List of directories containing Markdown files.
    - metadata_dict (dict): Mapping of Markdown file paths to their corresponding metadata.
    """
    # Loop through each directory provided

    all_files = get_files(markdown_dirs, ignored="error/")

    # Loop through each markdown file in the directory
    for file_path in all_files:
        if file_path.endswith(".md"):
            if file_path in metadata_dict:  # Check if full path is in metadata_dict
                # Extract metadata
                metadata = metadata_dict[file_path]

                # Open the markdown file, remove existing YAML front matter, and prepend new metadata
                with open(file_path, "r+", encoding="utf-8") as file:
                    content = file.read()
                    # Remove any existing front matter
                    content_without_frontmatter = remove_existing_yaml_frontmatter(content)
                    # Prepare the new YAML front matter
                    yaml_metadata = yaml.dump(metadata, default_flow_style=False, allow_unicode=True)
                    front_matter = f"---\n{yaml_metadata}---\n"
                    # Write the new front matter and content back to the file
                    file.seek(0, 0)
                    file.write(front_matter + content_without_frontmatter)
                    file.truncate()  # Ensure the file doesn't retain any old content beyond the new content
                print(f"Metadata attached to {file_path}")
            else:
                print(f"No metadata found for {file_path}. Skipping.")


def process_file(file_path, out_folder, stats, empty_llamaparse_files_counted, detailed_log_path, url=None):
    """
    Processes a file based on its extension: PDF or HTML.
    """

    txt_file_path = ""
    title_tag = ""
    # get the file extension
    file_extension = os.path.splitext(file_path)[1]

    if file_path.lower().endswith(".pdf"):
        # Handle PDF file
        stats["documents_sent_to_llamaparse"] += 1
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "stage": "parse",
            "filepath": file_path,
            "status": "PDF_PROCESSING_ATTEMPT",
            "reason": "Attempting to process PDF file.",
        }
        if detailed_log_path:
            with open(detailed_log_path, "a") as f:
                f.write(json.dumps(log_entry) + "\n")
        for i in range(3):
            if i > 0:
                log_entry = {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "stage": "parse",
                    "filepath": file_path,
                    "status": "PDF_RETRY",
                    "reason": f"Retrying PDF processing (attempt {i + 1}).",
                }
                if detailed_log_path:
                    with open(detailed_log_path, "a") as f:
                        f.write(json.dumps(log_entry) + "\n")
            txt_file_path = parse_pdf_to_txt(file_path, out_folder)
            if txt_file_path != "Error":
                log_entry = {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "stage": "parse",
                    "filepath": file_path,
                    "status": "PDF_TO_TXT_SUCCESS",
                    "reason": "Successfully converted PDF to TXT.",
                }
                if detailed_log_path:
                    with open(detailed_log_path, "a") as f:
                        f.write(json.dumps(log_entry) + "\n")
                break
            print("Error parsing PDF file. Retrying...")
            log_entry = {
                "timestamp": datetime.datetime.now().isoformat(),
                "stage": "parse",
                "filepath": file_path,
                "status": "PDF_TO_TXT_FAILED",
                "reason": "Failed to convert PDF to TXT. Retrying.",
            }
            if detailed_log_path:
                with open(detailed_log_path, "a") as f:
                    f.write(json.dumps(log_entry) + "\n")
            time.sleep(4)

        # If PDF parsing failed after all retries
        if txt_file_path == "Error":
            import pandas as pd

            # Log to error.csv
            error_data = [{
                "filepath": file_path,
                "URL": url if url else "N/A",
                "error_type": "PDF_PARSING_FAILED",
                "timestamp": datetime.datetime.now().isoformat()
            }]
            error_df = pd.DataFrame(error_data)
            data_path = os.getenv("DATA_PATH")
            error_csv_path = os.path.join(data_path, "error", "error.csv")
            os.makedirs(os.path.dirname(error_csv_path), exist_ok=True)

            with open(error_csv_path, "a") as f:
                f.write("\nPDF Parsing Failures\n")
            error_df.to_csv(error_csv_path, mode="a", index=False, header=True)

            # Move file to error folder in the crawl directory
            error_folder = os.path.join(os.path.dirname(file_path), "error")
            os.makedirs(error_folder, exist_ok=True)
            error_file_path = os.path.join(error_folder, os.path.basename(file_path))
            shutil.move(file_path, error_file_path)

            # Update stats and log
            stats["documents_failed_after_retries"] += 1
            log_entry = {
                "timestamp": datetime.datetime.now().isoformat(),
                "stage": "parse",
                "filepath": file_path,
                "status": "PDF_PARSING_FAILED_MOVED_TO_ERROR",
                "reason": "PDF parsing failed after 3 retries. File moved to error folder.",
            }
            if detailed_log_path:
                with open(detailed_log_path, "a") as f:
                    f.write(json.dumps(log_entry) + "\n")

            print(f"PDF parsing failed. Moved to {error_folder}")
            return  # Continue to next file

    elif file_path.lower().endswith(".html"):
        # Handle HTML file
        stats["documents_sent_to_llamaparse"] += 1
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "stage": "parse",
            "filepath": file_path,
            "status": "HTML_PROCESSING_ATTEMPT",
            "reason": "Attempting to process HTML file.",
        }
        if detailed_log_path:
            with open(detailed_log_path, "a") as f:
                f.write(json.dumps(log_entry) + "\n")
        for i in range(3):
            if i > 0:
                log_entry = {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "stage": "parse",
                    "filepath": file_path,
                    "status": "HTML_RETRY",
                    "reason": f"Retrying HTML processing (attempt {i + 1}).",
                }
                if detailed_log_path:
                    with open(detailed_log_path, "a") as f:
                        f.write(json.dumps(log_entry) + "\n")
            txt_file_path, title_tag = convert_html_to_markdown(file_path, out_folder)
            if title_tag != "Error parsing.":
                log_entry = {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "stage": "parse",
                    "filepath": file_path,
                    "status": "HTML_TO_TXT_SUCCESS",
                    "reason": "Successfully converted HTML to TXT.",
                }
                if detailed_log_path:
                    with open(detailed_log_path, "a") as f:
                        f.write(json.dumps(log_entry) + "\n")
                break
            print("Error converting HTML file. Retrying...")
            log_entry = {
                "timestamp": datetime.datetime.now().isoformat(),
                "stage": "parse",
                "filepath": file_path,
                "status": "HTML_TO_TXT_FAILED",
                "reason": "Failed to convert HTML to TXT. Retrying.",
            }
            if detailed_log_path:
                with open(detailed_log_path, "a") as f:
                    f.write(json.dumps(log_entry) + "\n")
            time.sleep(4)

        # If HTML parsing failed after all retries
        if title_tag == "Error parsing.":
            import pandas as pd

            # Log to error.csv
            error_data = [{
                "filepath": file_path,
                "URL": url if url else "N/A",
                "error_type": "HTML_PARSING_FAILED",
                "timestamp": datetime.datetime.now().isoformat()
            }]
            error_df = pd.DataFrame(error_data)
            data_path = os.getenv("DATA_PATH")
            error_csv_path = os.path.join(data_path, "error", "error.csv")
            os.makedirs(os.path.dirname(error_csv_path), exist_ok=True)

            with open(error_csv_path, "a") as f:
                f.write("\nHTML Parsing Failures\n")
            error_df.to_csv(error_csv_path, mode="a", index=False, header=True)

            # Move file to error folder in the crawl directory
            error_folder = os.path.join(os.path.dirname(file_path), "error")
            os.makedirs(error_folder, exist_ok=True)
            error_file_path = os.path.join(error_folder, os.path.basename(file_path))
            shutil.move(file_path, error_file_path)

            # Update stats and log
            stats["documents_failed_after_retries"] += 1
            log_entry = {
                "timestamp": datetime.datetime.now().isoformat(),
                "stage": "parse",
                "filepath": file_path,
                "status": "HTML_PARSING_FAILED_MOVED_TO_ERROR",
                "reason": "HTML parsing failed after 3 retries. File moved to error folder.",
            }
            if detailed_log_path:
                with open(detailed_log_path, "a") as f:
                    f.write(json.dumps(log_entry) + "\n")

            print(f"HTML parsing failed. Moved to {error_folder}")
            return  # Continue to next file

    if title_tag != "Error parsing." and txt_file_path != "Error":
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "stage": "parse",
            "filepath": file_path,
            "status": "LLAMAPARSE_ATTEMPT",
            "reason": "Attempting LlamaParse conversion.",
        }
        if detailed_log_path:
            with open(detailed_log_path, "a") as f:
                f.write(json.dumps(log_entry) + "\n")
        # try a maximum of 3 times to parse the txt file to md
        for i in range(3):
            is_empty = parse_txt_to_md(
                txt_file_path, file_extension, stats, empty_llamaparse_files_counted, detailed_log_path, title_tag, url
            )
            if not is_empty:
                # remove the txt file
                os.remove(txt_file_path)
                stats["documents_successful_after_retries"] += 1
                log_entry = {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "stage": "parse",
                    "filepath": file_path,
                    "status": "LLAMAPARSE_SUCCESS_OR_RETRY_SUCCEEDED",
                    "reason": "LlamaParse produced content or retry was successful.",
                }
                if detailed_log_path:
                    with open(detailed_log_path, "a") as f:
                        f.write(json.dumps(log_entry) + "\n")
                return
            print("Error parsing TXT file to MD. Retrying...")
            log_entry = {
                "timestamp": datetime.datetime.now().isoformat(),
                "stage": "parse",
                "filepath": file_path,
                "status": "LLAMAPARSE_EMPTY_RETRY",
                "reason": f"LlamaParse returned empty content. Retrying (attempt {i + 1}).",
            }
            if detailed_log_path:
                with open(detailed_log_path, "a") as f:
                    f.write(json.dumps(log_entry) + "\n")
            time.sleep(4)

    stats["documents_failed_after_retries"] += 1
    log_entry = {
        "timestamp": datetime.datetime.now().isoformat(),
        "stage": "parse",
        "filepath": file_path,
        "status": "FAILED_AFTER_ALL_RETRIES",
        "reason": "Document could not be processed after all LlamaParse retries.",
    }
    if detailed_log_path:
        with open(detailed_log_path, "a") as f:
            f.write(json.dumps(log_entry) + "\n")
    # move the txt file to the error folder
    error_folder = os.path.join(out_folder, "error")
    os.rename(txt_file_path, os.path.join(error_folder, os.path.basename(txt_file_path)))  # moving the file
    print(f"Error parsing TXT file to MD. Moved to {error_folder}")


def process_directory(origin_path, out_folder, stats, empty_llamaparse_files_counted, detailed_log_path):
    """
    Processes all HTML and PDF files in the specified directory.
    """
    import csv

    # Load all_links.csv for URL lookup
    all_links_path = os.path.join(os.path.dirname(origin_path), "all_links.csv")
    file_url_map = {}
    if os.path.exists(all_links_path):
        with open(all_links_path, newline="", encoding="utf-8") as file:
            reader = csv.DictReader(file)
            for row in reader:
                filename_with_ext = os.path.basename(row["filename"])
                filename_without_ext = os.path.splitext(filename_with_ext)[0]
                file_url_map[filename_without_ext] = row.get("URL")
    files_processed_by_directory = 0
    for root, _dirs, files in os.walk(origin_path):
        if "error" in root:
            continue
        for file in files:
            if file.lower().endswith((".html", ".pdf")):
                file_path = os.path.join(root, file)
                filename_without_ext = os.path.splitext(os.path.basename(file_path))[0]
                url = file_url_map.get(filename_without_ext)
                print(f"Processing file: {file_path} (URL: {url})")
                process_file(file_path, out_folder, stats, empty_llamaparse_files_counted, detailed_log_path, url=url)
                files_processed_by_directory += 1
    return files_processed_by_directory


def add_titles_tag(input_directory, out_folder):
    all_files = get_files(input_directory)
    # save only html files
    html_files = [file for file in all_files if file.endswith(".html")]
    # ignore the error folder
    html_files = [file for file in html_files if "error" not in file]
    out_files = get_files(out_folder)

    print(f"=== input directory: {input_directory}===")
    # Load a soup object from each html, get the title, and add it to the first line of the markdown file
    for file_path in html_files:
        with open(file_path, encoding="utf-8") as file:
            content = file.read()
        soup = BeautifulSoup(content, "html.parser")
        title = soup.title.string if soup.title else ""
        title = clean_title(title)

        if not title:
            continue

        print(f"title exist in {file_path}")

        # get the markdown file by filename
        filename = os.path.basename(file_path).replace(".html", ".md")
        md_file = [file for file in out_files if filename in file]

        if len(md_file) == 0:
            print(f"Markdown file not found for {filename}")
            continue
        # open the file
        with open(md_file[0], encoding="utf-8") as file:
            content = file.read()

        with open(md_file[0], "w", encoding="utf-8") as f:
            f.write(f"title: {title}\n")

            f.write(content)

        print(f"Title added to {filename}")
        print()
